---
page_title: Terraform Cloud Kubernetes Operator v2 Migration Guide
description: >-
  Upgrade the Terraform Kubernetes Operator from version 1 to version 2.
---

# Terraform Cloud Kubernetes Operator v2 Migration Guide

~> **Compatibility warning**: Terraform Enterprise only supports version 2 of the Terraform Cloud Kubernetes Operator. If you use Terraform Enterprise, [refer to this tutorial](/terraform/tutorials/kubernetes/kubernetes-operator-v2) for installation guidance. 

To upgrade the Terraform Cloud Kubernetes Operator from version 1 to the Terraform Cloud Kubernetes Operator (version 2), there is a one-time process that you need to complete. This process upgrades the operator to the newest version and migrate your custom resources. 

## Prerequisites

The migration process requires the following tools to be installed locally:

- [kubectl](https://kubernetes.io/docs/tasks/tools/#kubectl)
- [Helm](https://helm.sh/docs/intro/install/)

## Prepare for the upgrade

Configure an environment variable named `RELEASE_NAMESPACE` with the value of the namespace that the Helm chart is installed in.

```shell-session
$ export RELEASE_NAMESPACE=<NAMESPACE>
```

Next, create an environment variable named `RELEASE_NAME` with the value of the name that you gave your installation for the Helm chart.

```shell-session
$ export RELEASE_NAME=<INSTALLATION_NAME>
```

Before you migrate to Terraform Cloud Kubernetes Operator v2, you must first update v1 of the operator to the latest version, including the custom resource definitions.

```shell-session
$ helm upgrade --namespace ${RELEASE_NAMESPACE} ${RELEASE_NAME} hashicorp/terraform
```

Next, backup the workspace resources.

```shell-session
$ kubectl get workspace --all-namespaces -o yaml > backup_tfc_operator_v1.yaml
```

Finally, we suggest that you set the `applyMethod` of each HCP Terraform workspace to `manual`. In version 2 of the Terraform Cloud Kubernetes Operator this is the default value, however explicitly configuring it with the value provides extra protection from unintended automatic applies during the upgrade process. Run the following command for each Workspace resource.

```shell-session
kubectl patch workspace <WORKSPACE_NAME> --type=merge --patch '{"spec": {"applyMethod": "manual"}}'
```

## Manifest schema migration

### Workspace controller

In the table below you can find field mapping between v1 and v2.

| Version 1 | Version 2 | Notes |
| --- | --- | --- |
| `apiVersion: app.terraform.io/v1alpha1` | `apiVersion: app.terraform.io/v1alpha2` | CHANGED. In v2 the `apiVersion` changed from `v1alpha1` to `v1alpha2`. |
| `kind: Workspace` | `kind: Workspace` | DID NOT CHANGE. |
| `metadata` | `metadata` | DID NOT CHANGE. Entire `metadata` section remained unchanged. |
| `spec.organization` | `spec.organization` | DID NOT CHANGE. |
| `spec.secretsMountPath` | `spec.token.secretKeyRef` | CHANGED. In v2 the operator keeps HCP Terraform access token in a Kubernetes Secret. |
| `spec.vcs` | `spec.versionControl` | CHANGED. This field got a new name. See below changes inside the `versionControl` structure. |
| `spec.vcs.token_id` | `spec.versionControl.oAuthTokenID` | CHANGED. This field got a new name. |
| `spec.vcs.repo_identifier` | `spec.versionControl.repository` | CHANGED. This field got a new name. |
| `spec.vcs.branch` | `spec.versionControl.branch` | DID NOT CHANGE. |
| `spec.vcs.ingress_submodules` | `spec.workingDirectory` | CHANGED. This moved to a different level. |
| `spec.variables.[*]` | `spec.environmentVariables.[*]` OR `spec.terraformVariables.[*]` | CHANGED. Variables were split into two different paths. It depends on the bool value `spec.variables.environmentVariable` in v1 CRD:<br/> - If it is `true` then a variable should be migrated to `spec.environmentVariables`.<br/> - If it is `false` then a variable should be migrated to `spec.terraformVariables`. |
| `spec.variables.[*]key` | `spec.environmentVariables.[*]name` OR `spec.terraformVariables.[*]name` | CHANGED. In v2 `key` renamed to `name`. |
| `spec.variables.[*]value` | `spec.environmentVariables.[*]value` OR `spec.terraformVariables.[*]value` | DID NOT CHANGE. |
| `spec.variables.[*]valueFrom` | `spec.environmentVariables.[*]valueFrom` OR `spec.terraformVariables.[*]valueFrom` | DID NOT CHANGE. |
| `spec.variables.[*]hcl` | `spec.environmentVariables.[*]hcl` OR `spec.terraformVariables.[*]hcl` | DID NOT CHANGE. |
| `spec.variables.sensitive` |  `spec.environmentVariables.[*]sensitive` OR `spec.terraformVariables.[*]sensitive` | DID NOT CHANGE. |
| `spec.variables.environmentVariable` | N/A. | REMOVED. In v2 `environmentVariable` was removed and variables were split into two different paths. |
| `spec.runTriggers.[*]` | `spec.runTriggers.[*]` | DID NOT CHANGE. Changes were made inside the `runTriggers` structure, see below. |
| `spec.runTriggers.[*].sourceableName` | `spec.runTriggers.[*].name` | CHANGED. In v2 `sourceableName` was renamed to `name`. |
| `spec.sshKeyID` | `spec.sshKey.id` | CHANGED. In v2 `sshKeyID` was moved to `spec.sshKey.id`. |
| `spec.outputs` | N/A. | REMOVED. In v2 `spec.outputs` does not exist. |
| `spec.terraformVersion` | `spec.terraformVersion` | DID NOT CHANGE. |
| `spec.notifications` | `spec.notifications` | DID NOT CHANGE. Changes were made inside the `notification` structure, see below. |
| `spec.notifications.type` | `spec.notifications.type` | DID NOT CHANGE. |
| `spec.notifications.enabled` | `spec.notifications.enabled` | DID NOT CHANGE. |
| `spec.notifications.name` | `spec.notifications.name` | DID NOT CHANGE. |
| `spec.notifications.url` | `spec.notifications.url` | DID NOT CHANGE. |
| `spec.notifications.token` | `spec.notifications.token` | DID NOT CHANGE. |
| `spec.notifications.triggers.[*]` | `spec.notifications.triggers.[*]` | DID NOT CHANGE. |
| `spec.notifications.recepients.[*]` | `spec.notifications.emailAddesses.[*]` | CHANGED. In v2 `recepients` was changed to `emailAddesses`. |
| `spec.notifications.users.[*]` | `spec.notifications.emailUsers.[*]` | CHANGED. In v2 `users`was changed to `emailUsers`. |
| `spec.omitNamespacePrefix` | N/A. | REMOVED. In v1 `spec.omitNamespacePrefix` is a boolean field and affects a Workspace name:<br/> - If it is `true` then a Workspace name will be generated as `metadata.namespace-metadata.name`.<br/> - If it is `false` then a Workspace name will be generated as `metadata.name`.<br/> In v2 a Workspace name must be explicitly set in `spec.name`. |
| `spec.agentPoolID` | `spec.agentPool.id` | CHANGED. In v2 `agentPoolID` was moved to `spec.agentPool.id`. |
| `spec.agentPoolName` | `spec.agentPool.name` | CHANGED. In v2 `agentPoolName` was moved to `spec.agentPool.name`. |
| `spec.module` | N/A. | REMOVED. In v2 Module is a separate controller with its own CRD. See below the migration mapping. |

Here is an example of variables migration.

- Version 1.

  ```yaml
  apiVersion: app.terraform.io/v1alpha1
  kind: Workspace
  metadata:
    name: migration
    spec:
      variables:
        - key: username
          value: "user"
          hcl: true
          sensitive: false
          environmentVariable: false
        - key: SECRET_KEY
          value: "s3cr3t"
          hcl: false
          sensitive: false
          environmentVariable: true
  ```

- Version 2.

  ```yaml
  apiVersion: app.terraform.io/v1alpha2
  kind: Workspace
  metadata:
    name: migration
    spec:
      terraformVariables:
        - name: username
          value: "user"
          hcl: true
          sensitive: false
      environmentVariables:
        - name: SECRET_KEY
          value: "s3cr3t"
          hcl: false
          sensitive: false
  ```

### Module controller

Module controller is a new controller in v2 that aim to separate API-driven workflow from Workspace management. Below is a teamplate of a custom resource manifest:

```yaml
---
apiVersion: app.terraform.io/v1alpha2
kind: Module
metadata:
  name: <NAME>
spec:
  organization: <ORG-NAME>
  token:
    secretKeyRef:
      name: <SECRET-NAME>
      key: <KEY-NAME>
  name: operator
```

In the table below you can find field mapping between v1(Workspace controller) and v2(Module controller).

| Version 1 [Workspace CRD] | Version 2 [Module CRD] | Notes |
| --- | --- | --- |
| `spec.module` | N/A. | In v2 Module is a separate controller with its own CRD. |
| N/A. | `spec.name: operator` | In v1 name of the generated module is hardcoded to `opetator`. In v2 default name of the generated module is `this`, however, it can be changed. That decision was made intentionally to make the Module controller more flexible. This change can now be used to migrate existing API-driven workflows to the Operator. |
| `spec.module.source` | `spec.module.source` | This supports all Terraform [module sources](https://developer.hashicorp.com/terraform/language/modules/sources). |
| `spec.module.version` | `spec.module.version` | The module version. It works in the same way as in any other Terraform [module sources](https://developer.hashicorp.com/terraform/language/modules/sources). |
| `spec.variables.[*]` | `spec.variables.[*].name` | Variable names should be included in the module. This is a reference to variables in the Workspace where the module will be executed. |
| `spec.outputs.[*].key` | `spec.outputs.[*].name` | Output names should be included in the module. This is a reference to the output variables produced by the module. |
| `status.workspaceID` OR `metadata.namespace-metadata.name` / `metadata.name` – depends on `spec.omitNamespacePrefix` | `spec.workspace.id` OR `spec.workspace.name` | Explicit reference to the Workspace where the module will be executed. It doesn’t have to be the one managed by the Operator. Reference is possible by Workspace ID or Name and must be within the same organization. |

Here is an example of module migration.

- Version 1.

  ```yaml
  apiVersion: app.terraform.io/v1alpha1
  kind: Workspace
  metadata:
    name: migration
  spec:
    module:
      source: app.terraform.io/org-name/module-name/provider
      version: 0.0.42
    variables:
      - key: username
        value: "user"
        hcl: true
        sensitive: false
        environmentVariable: false
      - key: SECRET_KEY
        value: "s3cr3t"
        hcl: false
        sensitive: false
        environmentVariable: true
  ```

- Version 2.

  ```yaml
  apiVersion: app.terraform.io/v1alpha2
  kind: Workspace
  metadata:
    name: migration
    spec:
      terraformVariables:
        - name: username
          value: "user"
          hcl: true
          sensitive: false
      environmentVariables:
        - name: SECRET_KEY
          value: "s3cr3t"
          hcl: false
          sensitive: false
  ```

  ```yaml
  apiVersion: app.terraform.io/v1alpha2
  kind: Module
  metadata:
    name: migration
  spec:
    name: operator
    module:
      source: app.terraform.io/org-name/module-name/provider
      version: 0.0.42
    workspace:
      name: migration
  ```

## Upgrade the operator

View the changes that **patch A** will apply to the workspace CRD.

```shell-session
$ kubectl diff -f https://raw.githubusercontent.com/hashicorp/terraform-cloud-operator/main/docs/migration/crds/workspaces_patch_a.yaml
```

Patch the workspace CRD with **patch A**. This patch adds `app.terraform.io/v1alpha2` support, but excludes `.status.runStatus` because it has a different format in `app.terraform.io/v1alpha1` and causes JSON unmarshalling issues.

```shell-session
$ kubectl patch crd workspaces.app.terraform.io --patch-file https://raw.githubusercontent.com/hashicorp/terraform-cloud-operator/main/docs/migration/crds/workspaces_patch_a.yaml
```

Install the Operator v2 Helm chart with the `helm install` command. Be sure to set the `operator.watchedNamespaces` value to the list of namespaces your Workspace resources are deployed to. If this value is not provided, the operator will watch all namespaces in the Kubernetes cluster.

```shell-session
$ helm install \
  ${RELEASE_NAME} hashicorp/terraform-cloud-operator \
  --version 2.4.0 \
  --namespace ${RELEASE_NAMESPACE} \
  --set operator.syncPeriod=10m \
  --set 'operator.watchedNamespaces={white,blue,red}' \
  --set controllers.agentPool.workers=5 \
  --set controllers.module.workers=5 \
  --set controllers.workspace.workers=5
```

Next, create a Kubernetes secret to store the HCP Terraform API token following the [Usage Guide](https://github.com/hashicorp/terraform-cloud-operator/blob/main/docs/usage.md#prerequisites). The API token can be copied from the Kubernetes secret that you created for v1 of the operator. By default, this is named `terraformrc`. Use the `kubectl get secret` command to get the API token.

```shell-session
$ kubectl --namespace ${RELEASE_NAMESPACE} get secret terraformrc -o json | jq '.data.credentials' | tr -d '"' | base64 -d
```

If v2 of the operator pod fails to start, ensure that each `workspace` object has the `spec.token` value set. To do this, patch each `workspace` object using following command for each Workspace resource.

```shell-session
$ kubectl patch workspace <WORKSPACE_NAME> --type=merge --patch '{"spec": {"name": "NAME", "terraformVersion": "1.4.6", "token": {"secretKeyRef": {"key": "token", "name": "tfc-operator"}}}}'
```

Review the logs and verify that there are no error messages reported. The logs will report when the operator begins and finishes reconciling each workspace.

```shell-session
$ kubectl logs -f <POD_NAME>
INFO	Workspace Controller	{"workspace": {"name":"<WORKSPACE_NAME>","namespace":"<NAMESPACE>"}, "msg": "new reconciliation event"}
##...
INFO	Workspace Controller	{"workspace": {"name":"<WORKSPACE_NAME>","namespace":"<NAMESPACE>"}, "msg": "successfully reconciled workspace"}
```

View the changes that **patch B** will apply to the workspace CRD:

```shell-session
$ kubectl diff -f https://raw.githubusercontent.com/hashicorp/terraform-cloud-operator/main/docs/migration/crds/workspaces_patch_b.yaml
```

Patch workspace CRD with **patch B**. This patch adds `.status.runStatus` support that was excluded in Patch A.

```shell-session
$ kubectl patch crd workspaces.app.terraform.io --patch-file https://raw.githubusercontent.com/hashicorp/terraform-cloud-operator/main/docs/migration/crds/workspaces_patch_b.yaml
```

The v2 operator will fail to proceed if a custom resource has the v1 finalizer `finalizer.workspace.app.terraform.io`. If you encounter an error, check the logs for more information.

```shell-session
$ kubectl logs -f <POD_NAME>
```

Specifically, look for an error message such as the following.

```
ERROR	Migration	{"workspace": "default/<WORKSPACE_NAME>", "msg": "spec contains old finalizer finalizer.workspace.app.terraform.io"}
```
The `finalizer` exists to provide greater control over the migration process. Verify the custom resource, and when you’re ready to migrate it, use the `kubectl patch` command to update the `finalizer` value.

```shell-session
$ kubectl patch workspace migration --type=merge --patch '{"metadata": {"finalizers": ["workspace.app.terraform.io/finalizer"]}}'
```

Finally, review the operator logs once more and verify there are no error messages reported.

```shell-session
$ kubectl logs -f <POD_NAME>
```
